import streamlit as st
import pandas as pd
import yfinance as yf
import plotly.graph_objects as go

from modules.crawler import crawl_info_parallel
from modules.llm_handler import run_llm, run_llm_stock_analysis, run_llm_with_enhanced_content, check_vllm_server, test_vllm_simple
from modules.stock_analyzer import analyze_stock_characteristics, summarize_crawling_process, explain_llm_processing_logic
from modules.content_extractor import extract_content_from_url # Ï∂îÍ∞Ä
from config import NUM_REFERENCES, CONFIG_LANGUAGES

# UI Components

def header_ui():
    st.markdown("""
    <div style="background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); padding: 2rem; border-radius: 10px; margin-bottom: 2rem;">
        <h1 style="color: white; text-align: center; margin: 0;">üìà AI Í∏∞Î∞ò Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ Ï∂îÏ≤ú Î¶¨ÏÑúÏπò ÎèÑÏö∞ÎØ∏</h1>
        <p style="color: white; text-align: center; margin: 0.5rem 0 0 0;">S&P500 Ï¢ÖÎ™© Î∂ÑÏÑù Î∞è AI Í∏∞Î∞ò Ìà¨Ïûê ÏÇ¨Ïú† Î∂ÑÏÑù</p>
    </div>
    """, unsafe_allow_html=True)


def sidebar_ui(default_start, default_end):
    st.sidebar.header("üîß Î∂ÑÏÑù ÏÑ§Ï†ï")
    with st.sidebar.expander("üìÖ Ìà¨Ïûê Í∏∞Í∞Ñ ÏÑ§Ï†ï", expanded=True):
        start_date = st.date_input("Ìà¨Ïûê ÏãúÏûëÏùº:", value=default_start)
        end_date = st.date_input("Ìà¨Ïûê Ï¢ÖÎ£åÏùº:", value=default_end)
    with st.sidebar.expander("üéØ Î∂ÑÏÑù Ï°∞Í±¥", expanded=True):
        target_return = st.slider("Î™©Ìëú ÏàòÏùµÎ•† (%)", 0.0, 100.0, 10.0, 0.5)
        top_n = st.select_slider("Ï∂îÏ≤ú Ï¢ÖÎ™© Ïàò", list(range(1,21)), 5)
    # Language selection for LLM output
    language = st.sidebar.selectbox("Ïñ∏Ïñ¥ ÏÑ†ÌÉù:", options=CONFIG_LANGUAGES, index=0)
    analyze = st.sidebar.button("üöÄ Î∂ÑÏÑù Ïã§Ìñâ", type="primary")
    if analyze:
        st.session_state.analyze = True
    # vLLM ÏÑúÎ≤Ñ Ïó∞Í≤∞ ÏÉÅÌÉú ÌôïÏù∏ Î≤ÑÌäº
    if st.sidebar.button("üîß vLLM ÏÑúÎ≤Ñ ÏÉÅÌÉú ÌôïÏù∏"):
        from modules.llm_handler import check_vllm_server
        status, message = check_vllm_server()
        if status:
            st.sidebar.success(f"‚úÖ {message}")
        else:
            st.sidebar.error(f"‚ùå {message}")
    return start_date, end_date, target_return, top_n, language


def display_metrics(df):
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Î∂ÑÏÑùÎêú Ï¢ÖÎ™© Ïàò", len(df), delta="Í∞ú")
    with col2:
        st.metric("ÌèâÍ∑† ÏàòÏùµÎ•†", f"{df['Return (%)'].mean():.2f}%")
    with col3:
        st.metric("ÏµúÍ≥† ÏàòÏùµÎ•†", f"{df['Return (%)'].max():.2f}%")
    with col4:
        st.metric("ÌèâÍ∑† Î¶¨Ïä§ÌÅ¨", f"{df['Risk (%)'].mean():.2f}%")


def display_table_and_chart(stock_data, start_date, end_date):
    df = pd.DataFrame(stock_data)
    display_df = df.copy()
    display_df['Return (%)'] = display_df['Return (%)'].round(2)
    display_df['Risk (%)'] = display_df['Risk (%)'].round(2)
    col1, col2 = st.columns([1,2])
    with col1:
        st.subheader("üìä Ï¢ÖÎ™©Î≥Ñ ÏàòÏùµÎ•† & Î¶¨Ïä§ÌÅ¨")
        st.dataframe(display_df, use_container_width=True)
    with col2:
        st.subheader("üìà Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ Ï£ºÍ∞Ä Ï∞®Ìä∏")
        
        # Chart period selection
        chart_period = st.selectbox("Ï∞®Ìä∏ Ï£ºÍ∏∞ ÏÑ†ÌÉù", ["ÏùºÎ¥â","Ï£ºÎ¥â","ÏõîÎ¥â"], index=0, key="chart_period")
        
        # Moving Average trend line options
        st.write("**Ïù¥ÎèôÌèâÍ∑†ÏÑ† ÏÑ§Ï†ï:**")
        ma_cols = st.columns(4)
        show_ma = {}
        with ma_cols[0]:
            show_ma[5] = st.checkbox("MA5", value=True, key="ma5")
        with ma_cols[1]:
            show_ma[20] = st.checkbox("MA20", value=True, key="ma20")
        with ma_cols[2]:
            show_ma[60] = st.checkbox("MA60", value=False, key="ma60")
        with ma_cols[3]:
            show_ma[120] = st.checkbox("MA120", value=False, key="ma120")
        
        # Stock selection checkboxes
        st.write("**ÌëúÏãúÌï† Ï¢ÖÎ™© ÏÑ†ÌÉù:**")
        selected_stocks = []
        cols = st.columns(min(3, len(stock_data)))  # Create up to 3 columns
        
        for i, item in enumerate(stock_data):
            with cols[i % 3]:
                if st.checkbox(
                    item['Ticker'], 
                    value=True, 
                    key=f"stock_select_{item['Ticker']}"
                ):
                    selected_stocks.append(item)
        
        if not selected_stocks:
            st.warning("‚ö†Ô∏è Ï†ÅÏñ¥ÎèÑ ÌïòÎÇòÏùò Ï¢ÖÎ™©ÏùÑ ÏÑ†ÌÉùÌï¥Ï£ºÏÑ∏Ïöî.")
            return
            
        # Color palette for better visual separation
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']
            
        with st.spinner('üìà Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ ÏÑ±Í≥º Í≥ÑÏÇ∞ Ï§ë...'):
            fig = go.Figure()
            
            # Process each selected stock with unique colors
            for idx, item in enumerate(selected_stocks):
                base_color = colors[idx % len(colors)]
                data = yf.Ticker(item['Ticker']).history(start=str(start_date), end=str(end_date))
                
                # Check if data is available
                if data.empty:
                    st.warning(f"‚ö†Ô∏è {item['Ticker']} Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§.")
                    continue
                
                # Resample data based on selected period
                if chart_period == "Ï£ºÎ¥â":
                    # Use 'W-FRI' for weekly data ending on Friday
                    data = data.resample('W-FRI').agg({
                        'Open':'first',
                        'High':'max',
                        'Low':'min',
                        'Close':'last',
                        'Volume':'sum'
                    }).dropna()
                elif chart_period == "ÏõîÎ¥â":
                    # Use 'M' for monthly data ending on last day of month
                    data = data.resample('M').agg({
                        'Open':'first',
                        'High':'max',
                        'Low':'min',
                        'Close':'last',
                        'Volume':'sum'
                    }).dropna()
                
                # Check if resampled data is available
                if data.empty:
                    st.warning(f"‚ö†Ô∏è {item['Ticker']} {chart_period} Îç∞Ïù¥ÌÑ∞Í∞Ä Ï∂©Î∂ÑÌïòÏßÄ ÏïäÏäµÎãàÎã§.")
                    continue
                
                # Use Candlestick chart with custom colors
                fig.add_trace(go.Candlestick(
                    x=data.index,
                    open=data['Open'],
                    high=data['High'],
                    low=data['Low'],
                    close=data['Close'],
                    name=f"{item['Ticker']} Ï∫îÎì§",
                    increasing_line_color=base_color,
                    decreasing_line_color=base_color,
                    increasing_fillcolor=base_color,
                    decreasing_fillcolor=base_color,
                    opacity=0.8
                ))
                
                # Add Moving Average trend lines with coordinated colors
                ma_config = {
                    5: {'color': f"rgba({int(base_color[1:3], 16)}, {int(base_color[3:5], 16)}, {int(base_color[5:7], 16)}, 0.9)", 'style': 'solid', 'width': 1.5},
                    20: {'color': f"rgba({int(base_color[1:3], 16)}, {int(base_color[3:5], 16)}, {int(base_color[5:7], 16)}, 0.7)", 'style': 'dash', 'width': 2},
                    60: {'color': f"rgba({int(base_color[1:3], 16)}, {int(base_color[3:5], 16)}, {int(base_color[5:7], 16)}, 0.5)", 'style': 'dot', 'width': 2.5},
                    120: {'color': f"rgba({int(base_color[1:3], 16)}, {int(base_color[3:5], 16)}, {int(base_color[5:7], 16)}, 0.3)", 'style': 'dashdot', 'width': 3}
                }
                
                ma_names = {5: 'MA5', 20: 'MA20', 60: 'MA60', 120: 'MA120'}
                
                for period in [5, 20, 60, 120]:
                    if show_ma.get(period, False) and len(data) >= period:
                        ma_values = data['Close'].rolling(window=period).mean()
                        config = ma_config[period]
                        
                        fig.add_trace(go.Scatter(
                            x=data.index,
                            y=ma_values,
                            mode='lines',
                            name=f'{item["Ticker"]} {ma_names[period]}',
                            line=dict(
                                color=config['color'], 
                                width=config['width'], 
                                dash=config['style']
                            ),
                            opacity=0.8,
                            hovertemplate=f'<b>{item["Ticker"]} {ma_names[period]}</b><br>' +
                                        'Date: %{x}<br>' +
                                        f'{ma_names[period]}: $%{{y:.2f}}<br>' +
                                        '<extra></extra>'
                        ))
            fig.update_layout(
                title=f"{chart_period} Ï£ºÍ∞Ä Ï∞®Ìä∏ ({len(selected_stocks)}Í∞ú Ï¢ÖÎ™©)", 
                xaxis_title="ÎÇ†Ïßú", 
                yaxis_title="Ï£ºÍ∞Ä ($)", 
                hovermode='x unified', 
                height=800,  # Increased height for better visibility
                showlegend=True, 
                legend=dict(
                    orientation="v", 
                    yanchor="top", 
                    y=1, 
                    xanchor="left", 
                    x=1.01,
                    bgcolor="rgba(255,255,255,0.8)",
                    bordercolor="rgba(0,0,0,0.2)",
                    borderwidth=1
                ),
                margin=dict(r=200),  # Add right margin for legend
                xaxis=dict(
                    rangeslider=dict(visible=True, thickness=0.05),
                    rangeselector=dict(
                        buttons=list([
                            dict(count=7, label="7D", step="day", stepmode="backward"),
                            dict(count=30, label="1M", step="day", stepmode="backward"),
                            dict(count=90, label="3M", step="day", stepmode="backward"),
                            dict(count=180, label="6M", step="day", stepmode="backward"),
                            dict(label="Ï†ÑÏ≤¥", step="all")
                        ])
                    )
                ),
                dragmode='zoom',  # Enable zoom functionality
                plot_bgcolor='rgba(240,240,240,0.3)',  # Light background
                paper_bgcolor='white'
            )
            
            # Add custom controls description
            st.info("üí° **Ï∞®Ìä∏ Ï°∞ÏûëÎ≤ï:** ÎìúÎûòÍ∑∏Î°ú ÌôïÎåÄ, ÎçîÎ∏îÌÅ¥Î¶≠ÏúºÎ°ú Ï†ÑÏ≤¥Î≥¥Í∏∞, Î≤îÏúÑÏÑ†ÌÉù Î≤ÑÌäº ÌôúÏö©")
            st.plotly_chart(fig, use_container_width=True)


def display_risk_info():
    with st.expander("üìä Î¶¨Ïä§ÌÅ¨(Risk) Í≥ÑÏÇ∞ Î∞©Î≤ï", expanded=False):
        st.markdown("""
        **Î¶¨Ïä§ÌÅ¨Îäî Îã§ÏùåÍ≥º Í∞ôÏù¥ Í≥ÑÏÇ∞Îê©ÎãàÎã§:**
        1. **ÏùºÎ≥Ñ ÏàòÏùµÎ•† Í≥ÑÏÇ∞**: Ï¢ÖÍ∞Ä Í∏∞Ï§Ä ÏàòÏùµÎ•† Í≥ÑÏÇ∞
        2. **ÌëúÏ§ÄÌé∏Ï∞® Í≥ÑÏÇ∞**: ÏùºÎ≥Ñ ÏàòÏùµÎ•†Îì§Ïùò ÌëúÏ§ÄÌé∏Ï∞®
        3. **Î∞±Î∂ÑÏú® Î≥ÄÌôò**: ÌëúÏ§ÄÌé∏Ï∞®Ïóê 100 Í≥±Ìï®
        **Ìï¥ÏÑù:**
        - ÎÇÆÏùÄ Î¶¨Ïä§ÌÅ¨ (<2%): ÏïàÏ†ïÏ†Å
        - Ï§ëÍ∞Ñ Î¶¨Ïä§ÌÅ¨ (2-5%): Î≥¥ÌÜµ
        - ÎÜíÏùÄ Î¶¨Ïä§ÌÅ¨ (>5%): Î≥ÄÎèôÏÑ± ÎÜíÏùå
        """, unsafe_allow_html=True)


def display_ai_analysis(stock_data, start_date, language):
    st.markdown("---")
    st.header("ü§ñ AI Ìà¨Ïûê ÏÇ¨Ïú† Î∂ÑÏÑù")
    
    # vLLM ÏÑúÎ≤Ñ ÏÉÅÌÉú ÌôïÏù∏ - ÌÜ†Í∏ÄÎ°ú Î≥ÄÍ≤Ω
    with st.expander("üîß ÏãúÏä§ÌÖú ÏÉÅÌÉú ÌôïÏù∏", expanded=False):
        with st.spinner("vLLM ÏÑúÎ≤Ñ Ïó∞Í≤∞ ÌôïÏù∏ Ï§ë..." if language == "ÌïúÍµ≠Ïñ¥" else "Checking vLLM server connection..."):
            vllm_status, vllm_message = check_vllm_server()
            
            if vllm_status:
                success_msg = f"‚úÖ {vllm_message}" if language == "ÌïúÍµ≠Ïñ¥" else f"‚úÖ vLLM Server Connected: {vllm_message}"
                st.success(success_msg)
                
                # Í∞ÑÎã®Ìïú ÌÖåÏä§Ìä∏ ÏàòÌñâ
                test_status, test_message = test_vllm_simple()
                if test_status:
                    test_success_msg = f"‚úÖ vLLM ÌÖåÏä§Ìä∏: {test_message}" if language == "ÌïúÍµ≠Ïñ¥" else f"‚úÖ vLLM Test: {test_message}"
                    st.success(test_success_msg)
                else:
                    test_fail_msg = f"‚ö†Ô∏è vLLM ÌÖåÏä§Ìä∏ Ïã§Ìå®: {test_message}" if language == "ÌïúÍµ≠Ïñ¥" else f"‚ö†Ô∏è vLLM Test Failed: {test_message}"
                    st.warning(test_fail_msg)
            else:
                error_msg = f"‚ùå {vllm_message}" if language == "ÌïúÍµ≠Ïñ¥" else f"‚ùå vLLM Server Error: {vllm_message}"
                st.error(error_msg)
                
                if language == "ÌïúÍµ≠Ïñ¥":
                    st.error("vLLM ÏÑúÎ≤ÑÎ•º ÏãúÏûëÌï¥Ï£ºÏÑ∏Ïöî. ÌÑ∞ÎØ∏ÎÑêÏóêÏÑú Îã§Ïùå Î™ÖÎ†πÏñ¥Î•º Ïã§ÌñâÌïòÏÑ∏Ïöî:")
                    st.code("python -m vllm.entrypoints.openai.api_server --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 --port 8000")
                    st.info("üí° WSL ÌôòÍ≤ΩÏóêÏÑúÎäî scripts/wsl/start_vllm.sh Ïä§ÌÅ¨Î¶ΩÌä∏Î•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.")
                else:
                    st.error("Please start the vLLM server. Run the following command in terminal:")
                    st.code("python -m vllm.entrypoints.openai.api_server --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 --port 8000")
                    st.info("üí° For WSL environment, you can use the scripts/wsl/start_vllm.sh script.")
                st.stop()
    
    # LLM Ï≤òÎ¶¨ Î°úÏßÅ ÏÑ§Î™Ö
    with st.expander("üß† LLM Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ Î°úÏßÅ", expanded=False):
        llm_logic = explain_llm_processing_logic()
        
        st.subheader("üì• ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨")
        for step in llm_logic["input_processing"]:
            st.write(f"‚Ä¢ {step}")
        
        st.subheader("üîç Î∂ÑÏÑù Î∞©Î≤ïÎ°†")
        for method in llm_logic["analysis_methodology"]:
            st.write(f"‚Ä¢ {method}")
        
        st.subheader("üì§ Í≤∞Í≥º ÏÉùÏÑ±")
        for output in llm_logic["output_generation"]:
            st.write(f"‚Ä¢ {output}")
        
        st.subheader("‚úÖ ÌíàÏßà Î≥¥Ï¶ù")
        for qa in llm_logic["quality_assurance"]:
            st.write(f"‚Ä¢ {qa}")
    
    # ÎîîÎ≤ÑÍπÖ Ï†ïÎ≥¥ ÌëúÏãú
    with st.expander("üìà Ïã§ÏãúÍ∞Ñ Î∂ÑÏÑù Î°úÍ∑∏", expanded=False):
        debug_container = st.container()
    
    progress = st.progress(0)
    status = st.empty()
    reasons = []
    
    for i, item in enumerate(stock_data):
        ticker = item['Ticker']
        status.text(f"üîç {ticker} Î∂ÑÏÑù Ï§ë ({i+1}/{len(stock_data)})")
        progress.progress((i+1)/len(stock_data))
        
        with debug_container:
            st.info(f"üöÄ {ticker} ÌÅ¨Î°§ÎßÅ Î∞è LLM Î∂ÑÏÑù ÏãúÏûë...")
        
        try:
            # ÌÅ¨Î°§ÎßÅ Îã®Í≥Ñ ÎîîÎ≤ÑÍπÖ
            with debug_container:
                st.write(f"üì° {ticker} ÌÅ¨Î°§ÎßÅ Ï§ë...")
            articles, links, debug = crawl_info_parallel(ticker, start_date)
            
            with debug_container:
                st.success(f"‚úÖ {ticker} ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å: {len(articles)}Í∞ú Í∏∞ÏÇ¨ ÏàòÏßë")
                if len(articles) > 0:
                    st.write(f"Ï≤´ Î≤àÏß∏ Í∏∞ÏÇ¨ ÏÉòÌîå: {articles[0][:100]}...")
                
            # LLM Î∂ÑÏÑù Îã®Í≥Ñ ÎîîÎ≤ÑÍπÖ (Enhanced with link content)
            with debug_container:
                st.write(f"üß† {ticker} Í∞ïÌôîÎêú LLM Î∂ÑÏÑù Ï§ë...")
            
            # Use enhanced LLM analysis with link content extraction
            recommendation, review_content, extraction_debug = run_llm_with_enhanced_content(
                ticker, start_date, item['Return (%)'], articles, links, language
            )
            
            with debug_container:
                st.success(f"‚úÖ {ticker} Í∞ïÌôîÎêú LLM Î∂ÑÏÑù ÏôÑÎ£å")
                if recommendation:
                    st.write(f"LLM ÏùëÎãµ ÏÉòÌîå: {recommendation[:100]}...")
                    if extraction_debug:
                        st.write(f"ÎßÅÌÅ¨ Ï∂îÏ∂ú Ï†ïÎ≥¥: {len(extraction_debug)}Í∞ú ÎîîÎ≤ÑÍ∑∏ Ìï≠Î™©")
                else:
                    st.warning(f"‚ö†Ô∏è {ticker} LLM ÏùëÎãµÏù¥ ÎπÑÏñ¥ÏûàÏäµÎãàÎã§")
            
            # ÌÅ¨Î°§ÎßÅ ÏöîÏïΩ Î∞è Ï£ºÏãù Î∂ÑÏÑù Ï∂îÍ∞Ä
            crawling_summary = summarize_crawling_process(articles, debug)
            stock_analysis = analyze_stock_characteristics(
                ticker, 
                item['Return (%)'], 
                item['Risk (%)'], 
                start_date,
                '2024-12-31'
            )
        
        except Exception as e:
            with debug_container:
                st.error(f"‚ùå {ticker} Î∂ÑÏÑù Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}")
                st.write(f"Ïò§Î•ò ÏÑ∏Î∂ÄÏÇ¨Ìï≠: {type(e).__name__}")
            
            # Ïò§Î•ò Ïãú Í∏∞Î≥∏Í∞í ÏÑ§Ï†ï
            articles, links, debug = [], [], [f"Error: {str(e)}"]
            recommendation = f"Î∂ÑÏÑù Ïò§Î•ò Î∞úÏÉù: {str(e)}"
            review_content = recommendation
            extraction_debug = [f"Error during extraction: {str(e)}"]
            crawling_summary = {"data_quality": "Ïò§Î•ò", "total_sources": 0}
            stock_analysis = {"ticker": ticker, "investment_strategy": {"strategy": "Î∂ÑÏÑù Î∂àÍ∞Ä"}}
        
        # Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû•
        
        reasons.append({
            **item, 
            'Ï∂îÏ≤ú ÏÇ¨Ïú†': recommendation, 
            'ÌÅ¨Î°§ÎßÅ ÎîîÎ≤ÑÍ∑∏': debug, 
            'Ï∞∏Í≥† ÎßÅÌÅ¨': links, 
            'ÌÅ¨Î°§ÎßÅÎêú Í∏∞ÏÇ¨': articles, 
            'Î¶¨Î∑∞ ÎÇ¥Ïö©': review_content,
            'ÌÅ¨Î°§ÎßÅ ÏöîÏïΩ': crawling_summary,
            'Ï£ºÏãù Î∂ÑÏÑù': stock_analysis,
            'ÎßÅÌÅ¨ Ï∂îÏ∂ú ÎîîÎ≤ÑÍ∑∏': extraction_debug if 'extraction_debug' in locals() else []
        })
    progress.empty(); status.empty()
    
    # ÏàòÏßëÎêú Ï†ïÎ≥¥ ÏöîÏïΩ ÏÑπÏÖò Ï∂îÍ∞Ä
    with st.expander("üìã ÏàòÏßëÎêú Ï†ïÎ≥¥ ÏöîÏïΩ", expanded=True):
        generate_collected_info_summary(reasons, language, vllm_status)
    
    st.subheader("üìã Ï¢ÖÎ™© Ï∂îÏ≤ú ÏöîÏïΩ")
    for idx, r in enumerate(reasons):
        with st.expander(f"#{idx+1} {r['Ticker']} - ÏàòÏùµÎ•†: {r['Return (%)']:.2f}%", expanded= idx<3):
            c1, c2 = st.columns([1,3])
            with c1:
                st.metric("ÏàòÏùµÎ•†", f"{r['Return (%)']:.2f}%")
                st.metric("Î¶¨Ïä§ÌÅ¨", f"{r['Risk (%)']:.2f}%")
            with c2:
                st.write("**AI Î∂ÑÏÑù Í≤∞Í≥º:**")
                st.write(r['Ï∂îÏ≤ú ÏÇ¨Ïú†'])
                
                # Îî•Îü¨Îãù Î™®Îç∏ Ï∂îÏ≤ú ÏÑπÏÖò
                with st.expander("üß† Îî•Îü¨Îãù Î™®Îç∏ Ï∂îÏ≤ú", expanded=False):
                    analysis = r['Ï£ºÏãù Î∂ÑÏÑù']
                    st.write(f"**Ï∂îÏ≤ú Î™®Îç∏:** {analysis['primary_model']['model']}")
                    st.write(f"**ÏÑ†ÌÉù Ïù¥Ïú†:** {analysis['primary_model']['reason']}")
                    st.write(f"**ÎåÄÏïà Î™®Îç∏:** {analysis['secondary_model']['model']}")
                    st.write(f"**ÎåÄÏïà Ïù¥Ïú†:** {analysis['secondary_model']['reason']}")
                    
                    st.write("**Ìà¨Ïûê Ï†ÑÎûµ Ï†úÏïà:**")
                    strategy = analysis['investment_strategy']
                    st.write(f"‚Ä¢ **Ï†ÑÎûµ:** {strategy['strategy']}")
                    st.write(f"‚Ä¢ **ÏÑ§Î™Ö:** {strategy['description']}")
                    st.write(f"‚Ä¢ **Î™©Ìëú Í∏∞Í∞Ñ:** {strategy['target_period']}")
                    st.write(f"‚Ä¢ **Î¶¨Ïä§ÌÅ¨ Í¥ÄÎ¶¨:** {strategy['risk_management']}")
                
                # ÌÅ¨Î°§ÎßÅ Îç∞Ïù¥ÌÑ∞ ÏöîÏïΩ ÏÑπÏÖò
                with st.expander("üìä ÌÅ¨Î°§ÎßÅ Îç∞Ïù¥ÌÑ∞ ÏöîÏïΩ", expanded=False):
                    summary = r['ÌÅ¨Î°§ÎßÅ ÏöîÏïΩ']
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("Ï¥ù ÏÜåÏä§", summary.get('total_sources', 0))
                        st.metric("ÏÑ±Í≥µÌïú ÌÅ¨Î°§ÎßÅ", summary.get('successful_crawls', 0))
                        st.metric("Google Finance", summary.get('google_finance', 0))
                    with col2:
                        st.metric("Yahoo Finance", summary.get('yahoo_finance', 0))
                        st.metric("MarketWatch", summary.get('marketwatch', 0))
                        st.metric("RSS ÌîºÎìú", summary.get('rss_feeds', 0))
                    
                    st.write(f"**Îç∞Ïù¥ÌÑ∞ ÌíàÏßà:** {summary.get('data_quality', 'Ïïå Ïàò ÏóÜÏùå')}")
                    if summary.get('fallback_used'):
                        st.warning("‚ö†Ô∏è Ìè¥Î∞± ÏΩòÌÖêÏ∏†Í∞Ä ÏÇ¨Ïö©ÎêòÏóàÏäµÎãàÎã§")
                    
                    if 'article_breakdown' in summary:
                        st.write("**Í∏∞ÏÇ¨ Î∂ÑÏÑù:**")
                        for source, count in summary['article_breakdown'].items():
                            st.write(f"‚Ä¢ {source}: {count}Í∞ú")
                    
                    st.write("**Ï≤òÎ¶¨ Í≥ºÏ†ï:**")
                    for step in summary.get('processing_steps', []):
                        st.write(f"‚Ä¢ {step}")
                
                with st.expander("üîç ÌÅ¨Î°§ÎßÅ Í≤ÄÏ¶ù Ï†ïÎ≥¥", expanded=False):
                    st.write("**ÎîîÎ≤ÑÍ∑∏:**")
                    for d in r['ÌÅ¨Î°§ÎßÅ ÎîîÎ≤ÑÍ∑∏']:
                        st.text(d)
                    st.write("**ÌÅ¨Î°§ÎßÅÎêú Í∏∞ÏÇ¨ ÎÇ¥Ïö©:**")
                    for idx, art in enumerate(r['ÌÅ¨Î°§ÎßÅÎêú Í∏∞ÏÇ¨'][:NUM_REFERENCES]): 
                        st.write(f"**Í∏∞ÏÇ¨ {idx+1}:** {art[:200]}...")
                if r['Ï∞∏Í≥† ÎßÅÌÅ¨']:
                    st.write("**Ï∞∏Í≥† ÎßÅÌÅ¨:**")
                    for idx, l in enumerate(r['Ï∞∏Í≥† ÎßÅÌÅ¨'][:NUM_REFERENCES]): 
                        if l and l.startswith('http'):
                            st.markdown(f"üîó [Ï∞∏Í≥†ÏûêÎ£å {idx+1}]({l})")
                # LLMÏù¥ Î¶¨Î∑∞Ìïú ÎÇ¥Ïö© ÌÜ†Í∏ÄÎ°ú ÌëúÏãú
                with st.expander("üìù LLM Î¶¨Î∑∞ ÎÇ¥Ïö©", expanded=False):
                    st.write(r['Î¶¨Î∑∞ ÎÇ¥Ïö©'])
                
                # Í∞ïÌôîÎêú ÎßÅÌÅ¨ Î∂ÑÏÑù Ï†ïÎ≥¥ ÌëúÏãú
                if r.get('ÎßÅÌÅ¨ Ï∂îÏ∂ú ÎîîÎ≤ÑÍ∑∏'):
                    with st.expander("üîó Í∞ïÌôîÎêú ÎßÅÌÅ¨ Î∂ÑÏÑù", expanded=False):
                        st.write("**ÎßÅÌÅ¨ ÏΩòÌÖêÏ∏† Ï∂îÏ∂ú Í≥ºÏ†ï:**")
                        for debug_item in r['ÎßÅÌÅ¨ Ï∂îÏ∂ú ÎîîÎ≤ÑÍ∑∏']:
                            st.text(debug_item)
                        
                        st.info("üí° ÏÉÅÏúÑ Í¥ÄÎ†®ÏÑ± ÎßÅÌÅ¨ÏóêÏÑú Ï∂îÍ∞Ä ÏΩòÌÖêÏ∏†Î•º Ï∂îÏ∂úÌïòÏó¨ Îçî ÏÉÅÏÑ∏Ìïú Ìà¨Ïûê Î∂ÑÏÑùÏùÑ Ï†úÍ≥µÌñàÏäµÎãàÎã§.")
    
def display_crawling_test_ui(start_date, language):
    """UI for crawling test tab."""
    st.subheader("üîç ÌÅ¨Î°§ÎßÅ Í∏∞Îä• ÌÖåÏä§Ìä∏")
    
    # Ï¢ÖÎ™© Ìã∞Ïª§ Í∏∞Î∞ò ÌÖåÏä§Ìä∏
    st.write("**Ï¢ÖÎ™© Ìã∞Ïª§ Í∏∞Î∞ò ÌÅ¨Î°§ÎßÅ ÌÖåÏä§Ìä∏:**")
    if st.button("AAPL Ìã∞Ïª§Î°ú ÌÖåÏä§Ìä∏"):
        st.session_state.run_ticker_test = True
        st.session_state.ticker_test_results = None

    if st.session_state.get('run_ticker_test'):
        with st.spinner("AAPL ÌÅ¨Î°§ÎßÅ Ï§ë..."):
            try:
                articles, links, debug = crawl_info_parallel("AAPL", start_date)
                st.session_state.ticker_test_results = {"success": True, "articles": articles, "links": links, "debug": debug}
            except Exception as e:
                import traceback
                st.session_state.ticker_test_results = {"success": False, "error": str(e), "traceback": traceback.format_exc()}
        st.session_state.run_ticker_test = False
        st.rerun()

    if st.session_state.get('ticker_test_results'):
        res = st.session_state.ticker_test_results
        if res["success"]:
            st.success("‚úÖ AAPL ÌÅ¨Î°§ÎßÅ ÌÖåÏä§Ìä∏ ÏôÑÎ£å!")
            st.write(f"ÏàòÏßëÎêú Í∏∞ÏÇ¨: {len(res['articles'])}Í∞ú, ÎßÅÌÅ¨: {len(res['links'])}Í∞ú")
            with st.expander("ÏÉÅÏÑ∏ Í≤∞Í≥º Î≥¥Í∏∞"):
                st.json(res)
        else:
            st.error(f"‚ùå AAPL ÌÅ¨Î°§ÎßÅ ÌÖåÏä§Ìä∏ Ïã§Ìå®: {res['error']}")

    st.markdown("---")
    
    # URL Í∏∞Î∞ò ÏΩòÌÖêÏ∏† ÏöîÏïΩ ÌÖåÏä§Ìä∏
    st.write("**URL Í∏∞Î∞ò ÏΩòÌÖêÏ∏† ÏöîÏïΩ ÌÖåÏä§Ìä∏:**")
    test_url = st.text_input("ÌÖåÏä§Ìä∏Ìï† Í∏∞ÏÇ¨ URLÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî:", key="test_url")
    
    if st.button("URL ÏΩòÌÖêÏ∏† ÏöîÏïΩ Ïã§Ìñâ"):
        st.session_state.run_url_summary_test = True
        st.session_state.url_summary_results = None

    if st.session_state.get('run_url_summary_test') and test_url:
        with st.spinner("URL ÏΩòÌÖêÏ∏† Ï∂îÏ∂ú Î∞è ÏöîÏïΩ Ï§ë..."):
            try:
                # 1. ÏΩòÌÖêÏ∏† Ï∂îÏ∂ú
                content, _ = extract_content_from_url(test_url)
                
                # 2. LLMÏúºÎ°ú ÏöîÏïΩ
                if content:
                    if language == "ÌïúÍµ≠Ïñ¥":
                        summary_prompt = f"Îã§Ïùå Í∏∞ÏÇ¨Ïùò Ï£ºÏöî ÎÇ¥Ïö©ÏùÑ ÏÉÅÏÑ∏Ìûà ÌïúÍµ≠Ïñ¥Î°ú ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî. ÌïµÏã¨ Ìè¨Ïù∏Ìä∏, ÏàòÏπò, Ï§ëÏöîÌïú Ï†ïÎ≥¥Î•º Î™®Îëê Ìè¨Ìï®ÌïòÏó¨ Ï∂©Î∂ÑÌûà ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî:\n\n{content}"
                    else:
                        summary_prompt = f"Please provide a detailed summary of the following article in English. Include all key points, numbers, and important information with sufficient explanation:\n\n{content}"
                    
                    summary, _ = run_llm(summary_prompt, "", "URL ÏöîÏïΩ", language)
                    st.session_state.url_summary_results = {"success": True, "content": content, "summary": summary}
                else:
                    st.session_state.url_summary_results = {"success": False, "error": "ÏΩòÌÖêÏ∏†Î•º Ï∂îÏ∂úÌï† Ïàò ÏóÜÏäµÎãàÎã§."}

            except Exception as e:
                import traceback
                st.session_state.url_summary_results = {"success": False, "error": str(e), "traceback": traceback.format_exc()}
        st.session_state.run_url_summary_test = False
        st.rerun()

    if st.session_state.get('url_summary_results'):
        res = st.session_state.url_summary_results
        if res["success"]:
            st.success("‚úÖ URL ÏöîÏïΩ ÌÖåÏä§Ìä∏ ÏôÑÎ£å!")
            st.write("**ÏöîÏïΩ:**")
            st.write(res['summary'])
            with st.expander("Ï∂îÏ∂úÎêú ÏõêÎ¨∏ Î≥¥Í∏∞"):
                st.text(res['content'])
        else:
            st.error(f"‚ùå URL ÏöîÏïΩ ÌÖåÏä§Ìä∏ Ïã§Ìå®: {res['error']}")
            if "traceback" in res:
                st.text(res['traceback'])

def generate_collected_info_summary(reasons, language, vllm_status):
    """Generate a summary of collected information in the selected language."""
    
    if language == "ÌïúÍµ≠Ïñ¥":
        st.subheader("üìä Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ ÏàòÏßë ÌòÑÌô©")
        
        if not vllm_status:
            st.error("‚ö†Ô∏è vLLM ÏÑúÎ≤Ñ Ïó∞Í≤∞ Î¨∏Ï†úÎ°ú AI Î∂ÑÏÑùÏù¥ Ï†úÌïúÎê† Ïàò ÏûàÏäµÎãàÎã§. ÏãúÏä§ÌÖú ÏÉÅÌÉú ÌôïÏù∏ÏùÑ ÌÜµÌï¥ vLLM ÏÑúÎ≤ÑÎ•º ÏãúÏûëÌï¥Ï£ºÏÑ∏Ïöî.")
            st.info("üí° vLLM ÏÑúÎ≤Ñ ÏóÜÏù¥ÎèÑ ÌÅ¨Î°§ÎßÅÎêú Ï†ïÎ≥¥Îäî ÌôïÏù∏Ìï† Ïàò ÏûàÏßÄÎßå, AI Í∏∞Î∞ò Ìà¨Ïûê Î∂ÑÏÑùÏùÄ Ï†úÌïúÎê©ÎãàÎã§.")
        
        # Overall statistics
        total_stocks = len(reasons)
        total_articles = sum(len(r.get('ÌÅ¨Î°§ÎßÅÎêú Í∏∞ÏÇ¨', [])) for r in reasons)
        total_links = sum(len(r.get('Ï∞∏Í≥† ÎßÅÌÅ¨', [])) for r in reasons)
        successful_analyses = sum(1 for r in reasons if r.get('Ï∂îÏ≤ú ÏÇ¨Ïú†') and "Error" not in str(r.get('Ï∂îÏ≤ú ÏÇ¨Ïú†', '')))
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Î∂ÑÏÑù ÏôÑÎ£å Ï¢ÖÎ™©", f"{successful_analyses}/{total_stocks}")
        with col2:
            st.metric("ÏàòÏßëÎêú Í∏∞ÏÇ¨", total_articles)
        with col3:
            st.metric("Ï∞∏Í≥† ÎßÅÌÅ¨", total_links)
        with col4:
            success_rate = (successful_analyses / total_stocks * 100) if total_stocks > 0 else 0
            st.metric("ÏÑ±Í≥µÎ•†", f"{success_rate:.1f}%")
        
        # Data quality summary
        st.write("**üìà Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Î∂ÑÏÑù:**")
        quality_counts = {}
        for r in reasons:
            quality = r.get('ÌÅ¨Î°§ÎßÅ ÏöîÏïΩ', {}).get('data_quality', 'Ïïå Ïàò ÏóÜÏùå')
            quality_counts[quality] = quality_counts.get(quality, 0) + 1
        
        for quality, count in quality_counts.items():
            percentage = (count / total_stocks * 100) if total_stocks > 0 else 0
            st.write(f"‚Ä¢ {quality}: {count}Í∞ú Ï¢ÖÎ™© ({percentage:.1f}%)")
        
        # Source-wise collection status
        st.write("**üîç Ï†ïÎ≥¥ ÏÜåÏä§Î≥Ñ ÏàòÏßë ÌòÑÌô©:**")
        source_summary = {
            'Google Finance': 0, 'Yahoo Finance': 0, 'MarketWatch': 0, 
            'RSS ÌîºÎìú': 0, 'DuckDuckGo': 0
        }
        
        for r in reasons:
            crawling_summary = r.get('ÌÅ¨Î°§ÎßÅ ÏöîÏïΩ', {})
            for source in source_summary.keys():
                source_key = source.lower().replace(' ', '_')
                source_summary[source] += crawling_summary.get(source_key, 0)
        
        for source, count in source_summary.items():
            st.write(f"‚Ä¢ {source}: {count}Í∞ú Í∏∞ÏÇ¨")
        
        # Enhanced analysis information
        enhanced_count = sum(1 for r in reasons if r.get('ÎßÅÌÅ¨ Ï∂îÏ∂ú ÎîîÎ≤ÑÍ∑∏'))
        if enhanced_count > 0:
            st.write("**üîó Í∞ïÌôîÎêú ÎßÅÌÅ¨ Î∂ÑÏÑù:**")
            st.write(f"‚Ä¢ {enhanced_count}Í∞ú Ï¢ÖÎ™©ÏóêÏÑú Ï∂îÍ∞Ä ÎßÅÌÅ¨ ÏΩòÌÖêÏ∏† Î∂ÑÏÑù ÏôÑÎ£å")
            st.info("üí° ÏÉÅÏúÑ Í¥ÄÎ†®ÏÑ± ÎßÅÌÅ¨ÏóêÏÑú Ï∂îÏ∂úÌïú Ï∂îÍ∞Ä Ï†ïÎ≥¥Î°ú Îçî Ï†ïÌôïÌïú Ìà¨Ïûê Î∂ÑÏÑùÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.")
        
        # Summary of issues if any
        error_count = sum(1 for r in reasons if "Error" in str(r.get('Ï∂îÏ≤ú ÏÇ¨Ïú†', '')))
        if error_count > 0:
            st.warning(f"‚ö†Ô∏è {error_count}Í∞ú Ï¢ÖÎ™©ÏóêÏÑú Î∂ÑÏÑù Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.")
            if not vllm_status:
                st.error("üîß ÎåÄÎ∂ÄÎ∂ÑÏùò Ïò§Î•òÎäî vLLM ÏÑúÎ≤Ñ Ïó∞Í≤∞ Î¨∏Ï†úÎ°ú Ïù∏Ìïú Í≤ÉÏûÖÎãàÎã§. ÏãúÏä§ÌÖú ÏÉÅÌÉúÎ•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.")
        
    else:  # English
        st.subheader("üìä Overall Data Collection Status")
        
        if not vllm_status:
            st.error("‚ö†Ô∏è vLLM server connection issues limit AI analysis capabilities. Please check system status to start the vLLM server.")
            st.info("üí° Crawled information is available without vLLM server, but AI-based investment analysis is limited.")
        
        # Overall statistics
        total_stocks = len(reasons)
        total_articles = sum(len(r.get('ÌÅ¨Î°§ÎßÅÎêú Í∏∞ÏÇ¨', [])) for r in reasons)
        total_links = sum(len(r.get('Ï∞∏Í≥† ÎßÅÌÅ¨', [])) for r in reasons)
        successful_analyses = sum(1 for r in reasons if r.get('Ï∂îÏ≤ú ÏÇ¨Ïú†') and "Error" not in str(r.get('Ï∂îÏ≤ú ÏÇ¨Ïú†', '')))
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Analyzed Stocks", f"{successful_analyses}/{total_stocks}")
        with col2:
            st.metric("Articles Collected", total_articles)
        with col3:
            st.metric("Reference Links", total_links)
        with col4:
            success_rate = (successful_analyses / total_stocks * 100) if total_stocks > 0 else 0
            st.metric("Success Rate", f"{success_rate:.1f}%")
        
        # Data quality summary
        st.write("**üìà Data Quality Analysis:**")
        quality_counts = {}
        for r in reasons:
            quality = r.get('ÌÅ¨Î°§ÎßÅ ÏöîÏïΩ', {}).get('data_quality', 'Unknown')
            quality_counts[quality] = quality_counts.get(quality, 0) + 1
        
        for quality, count in quality_counts.items():
            percentage = (count / total_stocks * 100) if total_stocks > 0 else 0
            st.write(f"‚Ä¢ {quality}: {count} stocks ({percentage:.1f}%)")
        
        # Source-wise collection status
        st.write("**üîç Information Source Collection Status:**")
        source_summary = {
            'Google Finance': 0, 'Yahoo Finance': 0, 'MarketWatch': 0, 
            'RSS Feeds': 0, 'DuckDuckGo': 0
        }
        
        for r in reasons:
            crawling_summary = r.get('ÌÅ¨Î°§ÎßÅ ÏöîÏïΩ', {})
            source_mapping = {
                'Google Finance': 'google_finance',
                'Yahoo Finance': 'yahoo_finance', 
                'MarketWatch': 'marketwatch',
                'RSS Feeds': 'rss_feeds',
                'DuckDuckGo': 'duckduckgo'
            }
            for source, key in source_mapping.items():
                source_summary[source] += crawling_summary.get(key, 0)
        
        for source, count in source_summary.items():
            st.write(f"‚Ä¢ {source}: {count} articles")
        
        # Enhanced analysis information
        enhanced_count = sum(1 for r in reasons if r.get('ÎßÅÌÅ¨ Ï∂îÏ∂ú ÎîîÎ≤ÑÍ∑∏'))
        if enhanced_count > 0:
            st.write("**üîó Enhanced Link Analysis:**")
            st.write(f"‚Ä¢ {enhanced_count} stocks completed additional link content analysis")
            st.info("üí° Provides more accurate investment analysis with additional information extracted from top relevance links.")
        
        # Summary of issues if any
        error_count = sum(1 for r in reasons if "Error" in str(r.get('Ï∂îÏ≤ú ÏÇ¨Ïú†', '')))
        if error_count > 0:
            st.warning(f"‚ö†Ô∏è {error_count} stocks encountered analysis errors.")
            if not vllm_status:
                st.error("üîß Most errors are due to vLLM server connection issues. Please check system status.")
