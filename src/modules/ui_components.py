import streamlit as st
import pandas as pd
import yfinance as yf
import plotly.graph_objects as go

from modules.crawler import crawl_info_parallel
from modules.llm_handler import run_llm, run_llm_with_enhanced_content, check_vllm_server, test_vllm_simple
from modules.stock_analyzer import analyze_stock_characteristics, summarize_crawling_process, explain_llm_processing_logic
from config import NUM_REFERENCES, CONFIG_LANGUAGES

# UI Components

def header_ui():
    st.markdown("""
    <div style="background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); padding: 2rem; border-radius: 10px; margin-bottom: 2rem;">
        <h1 style="color: white; text-align: center; margin: 0;">ğŸ“ˆ AI ê¸°ë°˜ í¬íŠ¸í´ë¦¬ì˜¤ ì¶”ì²œ ë¦¬ì„œì¹˜ ë„ìš°ë¯¸</h1>
        <p style="color: white; text-align: center; margin: 0.5rem 0 0 0;">S&P500 ì¢…ëª© ë¶„ì„ ë° AI ê¸°ë°˜ íˆ¬ì ì‚¬ìœ  ë¶„ì„</p>
    </div>
    """, unsafe_allow_html=True)


def sidebar_ui(default_start, default_end):
    st.sidebar.header("ğŸ”§ ë¶„ì„ ì„¤ì •")
    with st.sidebar.expander("ğŸ“… íˆ¬ì ê¸°ê°„ ì„¤ì •", expanded=True):
        start_date = st.date_input("íˆ¬ì ì‹œì‘ì¼:", value=default_start)
        end_date = st.date_input("íˆ¬ì ì¢…ë£Œì¼:", value=default_end)
    with st.sidebar.expander("ğŸ¯ ë¶„ì„ ì¡°ê±´", expanded=True):
        target_return = st.slider("ëª©í‘œ ìˆ˜ìµë¥  (%)", 0.0, 100.0, 10.0, 0.5)
        top_n = st.select_slider("ì¶”ì²œ ì¢…ëª© ìˆ˜", list(range(1,21)), 5)
    # Language selection for LLM output
    language = st.sidebar.selectbox("ì–¸ì–´ ì„ íƒ:", options=CONFIG_LANGUAGES, index=0)
    analyze = st.sidebar.button("ğŸš€ ë¶„ì„ ì‹¤í–‰", type="primary")
    if analyze:
        st.session_state.analyze = True
    return start_date, end_date, target_return, top_n, language


def display_metrics(df):
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ë¶„ì„ëœ ì¢…ëª© ìˆ˜", len(df), delta="ê°œ")
    with col2:
        st.metric("í‰ê·  ìˆ˜ìµë¥ ", f"{df['Return (%)'].mean():.2f}%")
    with col3:
        st.metric("ìµœê³  ìˆ˜ìµë¥ ", f"{df['Return (%)'].max():.2f}%")
    with col4:
        st.metric("í‰ê·  ë¦¬ìŠ¤í¬", f"{df['Risk (%)'].mean():.2f}%")


def display_table_and_chart(stock_data, start_date, end_date):
    df = pd.DataFrame(stock_data)
    display_df = df.copy()
    display_df['Return (%)'] = display_df['Return (%)'].round(2)
    display_df['Risk (%)'] = display_df['Risk (%)'].round(2)
    col1, col2 = st.columns([1,2])
    with col1:
        st.subheader("ğŸ“Š ì¢…ëª©ë³„ ìˆ˜ìµë¥  & ë¦¬ìŠ¤í¬")
        st.dataframe(display_df, use_container_width=True)
    with col2:
        st.subheader("ğŸ“ˆ í¬íŠ¸í´ë¦¬ì˜¤ ëˆ„ì  ìˆ˜ìµë¥ ")
        chart_period = st.selectbox("ì°¨íŠ¸ ì£¼ê¸° ì„ íƒ", ["ì¼ë´‰","ì£¼ë´‰","ì›”ë´‰"], index=0)
        with st.spinner('ğŸ“ˆ í¬íŠ¸í´ë¦¬ì˜¤ ì„±ê³¼ ê³„ì‚° ì¤‘...'):
            fig = go.Figure()
            for item in stock_data:
                data = yf.Ticker(item['Ticker']).history(start=str(start_date), end=str(end_date))
                if chart_period == "ì£¼ë´‰":
                    data = data.resample('W').agg({'Open':'first','High':'max','Low':'min','Close':'last','Volume':'sum'}).dropna()
                elif chart_period == "ì›”ë´‰":
                    data = data.resample('M').agg({'Open':'first','High':'max','Low':'min','Close':'last','Volume':'sum'}).dropna()
                cumulative = (data['Close'].pct_change().add(1).cumprod() - 1) * 100  # % ë‹¨ìœ„ë¡œ ë³€í™˜
                fig.add_trace(go.Scatter(
                    x=cumulative.index, 
                    y=cumulative.values, 
                    mode='lines', 
                    name=item['Ticker'], 
                    line=dict(width=2),
                    hovertemplate='<b>%{fullData.name}</b><br>' +
                                'Date: %{x}<br>' +
                                'Return: %{y:.2f}%<br>' +
                                '<extra></extra>'
                ))
            fig.update_layout(
                title=f"{chart_period} ëˆ„ì  ìˆ˜ìµë¥  ì¶”ì´", 
                xaxis_title="ë‚ ì§œ", 
                yaxis_title="ëˆ„ì  ìˆ˜ìµë¥  (%)", 
                hovermode='x unified', 
                height=600, 
                showlegend=True, 
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
            )
            fig.update_xaxes(rangeslider_visible=True)
            st.plotly_chart(fig, use_container_width=True)


def display_risk_info():
    with st.expander("ğŸ“Š ë¦¬ìŠ¤í¬(Risk) ê³„ì‚° ë°©ë²•", expanded=False):
        st.markdown("""
        **ë¦¬ìŠ¤í¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤:**
        1. **ì¼ë³„ ìˆ˜ìµë¥  ê³„ì‚°**: ì¢…ê°€ ê¸°ì¤€ ìˆ˜ìµë¥  ê³„ì‚°
        2. **í‘œì¤€í¸ì°¨ ê³„ì‚°**: ì¼ë³„ ìˆ˜ìµë¥ ë“¤ì˜ í‘œì¤€í¸ì°¨
        3. **ë°±ë¶„ìœ¨ ë³€í™˜**: í‘œì¤€í¸ì°¨ì— 100 ê³±í•¨
        **í•´ì„:**
        - ë‚®ì€ ë¦¬ìŠ¤í¬ (<2%): ì•ˆì •ì 
        - ì¤‘ê°„ ë¦¬ìŠ¤í¬ (2-5%): ë³´í†µ
        - ë†’ì€ ë¦¬ìŠ¤í¬ (>5%): ë³€ë™ì„± ë†’ìŒ
        """, unsafe_allow_html=True)


def display_ai_analysis(stock_data, start_date, language):
    st.markdown("---")
    st.header("ğŸ¤– AI íˆ¬ì ì‚¬ìœ  ë¶„ì„")
    
    # vLLM ì„œë²„ ìƒíƒœ í™•ì¸ - í† ê¸€ë¡œ ë³€ê²½
    with st.expander("ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸", expanded=False):
        with st.spinner("vLLM ì„œë²„ ì—°ê²° í™•ì¸ ì¤‘..." if language == "í•œêµ­ì–´" else "Checking vLLM server connection..."):
            vllm_status, vllm_message = check_vllm_server()
            
            if vllm_status:
                success_msg = f"âœ… {vllm_message}" if language == "í•œêµ­ì–´" else f"âœ… vLLM Server Connected: {vllm_message}"
                st.success(success_msg)
                
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
                test_status, test_message = test_vllm_simple()
                if test_status:
                    test_success_msg = f"âœ… vLLM í…ŒìŠ¤íŠ¸: {test_message}" if language == "í•œêµ­ì–´" else f"âœ… vLLM Test: {test_message}"
                    st.success(test_success_msg)
                else:
                    test_fail_msg = f"âš ï¸ vLLM í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {test_message}" if language == "í•œêµ­ì–´" else f"âš ï¸ vLLM Test Failed: {test_message}"
                    st.warning(test_fail_msg)
            else:
                error_msg = f"âŒ {vllm_message}" if language == "í•œêµ­ì–´" else f"âŒ vLLM Server Error: {vllm_message}"
                st.error(error_msg)
                
                if language == "í•œêµ­ì–´":
                    st.error("vLLM ì„œë²„ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”. í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
                    st.code("python -m vllm.entrypoints.openai.api_server --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 --port 8000")
                    st.info("ğŸ’¡ WSL í™˜ê²½ì—ì„œëŠ” scripts/wsl/start_vllm.sh ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                else:
                    st.error("Please start the vLLM server. Run the following command in terminal:")
                    st.code("python -m vllm.entrypoints.openai.api_server --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 --port 8000")
                    st.info("ğŸ’¡ For WSL environment, you can use the scripts/wsl/start_vllm.sh script.")
                st.stop()
    
    # LLM ì²˜ë¦¬ ë¡œì§ ì„¤ëª…
    with st.expander("ğŸ§  LLM ë°ì´í„° ì²˜ë¦¬ ë¡œì§", expanded=False):
        llm_logic = explain_llm_processing_logic()
        
        st.subheader("ğŸ“¥ ì…ë ¥ ë°ì´í„° ì²˜ë¦¬")
        for step in llm_logic["input_processing"]:
            st.write(f"â€¢ {step}")
        
        st.subheader("ğŸ” ë¶„ì„ ë°©ë²•ë¡ ")
        for method in llm_logic["analysis_methodology"]:
            st.write(f"â€¢ {method}")
        
        st.subheader("ğŸ“¤ ê²°ê³¼ ìƒì„±")
        for output in llm_logic["output_generation"]:
            st.write(f"â€¢ {output}")
        
        st.subheader("âœ… í’ˆì§ˆ ë³´ì¦")
        for qa in llm_logic["quality_assurance"]:
            st.write(f"â€¢ {qa}")
    
    # ë””ë²„ê¹… ì •ë³´ í‘œì‹œ
    debug_container = st.container()
    
    progress = st.progress(0)
    status = st.empty()
    reasons = []
    
    for i, item in enumerate(stock_data):
        ticker = item['Ticker']
        status.text(f"ğŸ” {ticker} ë¶„ì„ ì¤‘ ({i+1}/{len(stock_data)})")
        progress.progress((i+1)/len(stock_data))
        
        with debug_container:
            st.info(f"ğŸš€ {ticker} í¬ë¡¤ë§ ë° LLM ë¶„ì„ ì‹œì‘...")
        
        try:
            # í¬ë¡¤ë§ ë‹¨ê³„ ë””ë²„ê¹…
            with debug_container:
                st.write(f"ğŸ“¡ {ticker} í¬ë¡¤ë§ ì¤‘...")
            articles, links, debug = crawl_info_parallel(ticker, start_date)
            
            with debug_container:
                st.success(f"âœ… {ticker} í¬ë¡¤ë§ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                if len(articles) > 0:
                    st.write(f"ì²« ë²ˆì§¸ ê¸°ì‚¬ ìƒ˜í”Œ: {articles[0][:100]}...")
                
            # LLM ë¶„ì„ ë‹¨ê³„ ë””ë²„ê¹… (Enhanced with link content)
            with debug_container:
                st.write(f"ğŸ§  {ticker} ê°•í™”ëœ LLM ë¶„ì„ ì¤‘...")
            
            # Use enhanced LLM analysis with link content extraction
            recommendation, review_content, extraction_debug = run_llm_with_enhanced_content(
                ticker, start_date, item['Return (%)'], articles, links, language
            )
            
            with debug_container:
                st.success(f"âœ… {ticker} ê°•í™”ëœ LLM ë¶„ì„ ì™„ë£Œ")
                if recommendation:
                    st.write(f"LLM ì‘ë‹µ ìƒ˜í”Œ: {recommendation[:100]}...")
                    if extraction_debug:
                        st.write(f"ë§í¬ ì¶”ì¶œ ì •ë³´: {len(extraction_debug)}ê°œ ë””ë²„ê·¸ í•­ëª©")
                else:
                    st.warning(f"âš ï¸ {ticker} LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            # í¬ë¡¤ë§ ìš”ì•½ ë° ì£¼ì‹ ë¶„ì„ ì¶”ê°€
            crawling_summary = summarize_crawling_process(articles, debug)
            stock_analysis = analyze_stock_characteristics(
                ticker, 
                item['Return (%)'], 
                item['Risk (%)'], 
                start_date,
                '2024-12-31'
            )
        
        except Exception as e:
            with debug_container:
                st.error(f"âŒ {ticker} ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                st.write(f"ì˜¤ë¥˜ ì„¸ë¶€ì‚¬í•­: {type(e).__name__}")
            
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ì„¤ì •
            articles, links, debug = [], [], [f"Error: {str(e)}"]
            recommendation = f"ë¶„ì„ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            review_content = recommendation
            extraction_debug = [f"Error during extraction: {str(e)}"]
            crawling_summary = {"data_quality": "ì˜¤ë¥˜", "total_sources": 0}
            stock_analysis = {"ticker": ticker, "investment_strategy": {"strategy": "ë¶„ì„ ë¶ˆê°€"}}
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        
        reasons.append({
            **item, 
            'ì¶”ì²œ ì‚¬ìœ ': recommendation, 
            'í¬ë¡¤ë§ ë””ë²„ê·¸': debug, 
            'ì°¸ê³  ë§í¬': links, 
            'í¬ë¡¤ë§ëœ ê¸°ì‚¬': articles, 
            'ë¦¬ë·° ë‚´ìš©': review_content,
            'í¬ë¡¤ë§ ìš”ì•½': crawling_summary,
            'ì£¼ì‹ ë¶„ì„': stock_analysis,
            'ë§í¬ ì¶”ì¶œ ë””ë²„ê·¸': extraction_debug if 'extraction_debug' in locals() else []
        })
    progress.empty(); status.empty()
    
    # ìˆ˜ì§‘ëœ ì •ë³´ ìš”ì•½ ì„¹ì…˜ ì¶”ê°€
    with st.expander("ğŸ“‹ ìˆ˜ì§‘ëœ ì •ë³´ ìš”ì•½", expanded=True):
        generate_collected_info_summary(reasons, language, vllm_status)
    
    st.subheader("ğŸ“‹ ì¢…ëª© ì¶”ì²œ ìš”ì•½")
    for idx, r in enumerate(reasons):
        with st.expander(f"#{idx+1} {r['Ticker']} - ìˆ˜ìµë¥ : {r['Return (%)']:.2f}%", expanded= idx<3):
            c1, c2 = st.columns([1,3])
            with c1:
                st.metric("ìˆ˜ìµë¥ ", f"{r['Return (%)']:.2f}%")
                st.metric("ë¦¬ìŠ¤í¬", f"{r['Risk (%)']:.2f}%")
            with c2:
                st.write("**AI ë¶„ì„ ê²°ê³¼:**")
                st.write(r['ì¶”ì²œ ì‚¬ìœ '])
                
                # ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¶”ì²œ ì„¹ì…˜
                with st.expander("ğŸ§  ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¶”ì²œ", expanded=False):
                    analysis = r['ì£¼ì‹ ë¶„ì„']
                    st.write(f"**ì¶”ì²œ ëª¨ë¸:** {analysis['primary_model']['model']}")
                    st.write(f"**ì„ íƒ ì´ìœ :** {analysis['primary_model']['reason']}")
                    st.write(f"**ëŒ€ì•ˆ ëª¨ë¸:** {analysis['secondary_model']['model']}")
                    st.write(f"**ëŒ€ì•ˆ ì´ìœ :** {analysis['secondary_model']['reason']}")
                    
                    st.write("**íˆ¬ì ì „ëµ ì œì•ˆ:**")
                    strategy = analysis['investment_strategy']
                    st.write(f"â€¢ **ì „ëµ:** {strategy['strategy']}")
                    st.write(f"â€¢ **ì„¤ëª…:** {strategy['description']}")
                    st.write(f"â€¢ **ëª©í‘œ ê¸°ê°„:** {strategy['target_period']}")
                    st.write(f"â€¢ **ë¦¬ìŠ¤í¬ ê´€ë¦¬:** {strategy['risk_management']}")
                
                # í¬ë¡¤ë§ ë°ì´í„° ìš”ì•½ ì„¹ì…˜
                with st.expander("ğŸ“Š í¬ë¡¤ë§ ë°ì´í„° ìš”ì•½", expanded=False):
                    summary = r['í¬ë¡¤ë§ ìš”ì•½']
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("ì´ ì†ŒìŠ¤", summary.get('total_sources', 0))
                        st.metric("ì„±ê³µí•œ í¬ë¡¤ë§", summary.get('successful_crawls', 0))
                        st.metric("Google Finance", summary.get('google_finance', 0))
                    with col2:
                        st.metric("Yahoo Finance", summary.get('yahoo_finance', 0))
                        st.metric("MarketWatch", summary.get('marketwatch', 0))
                        st.metric("RSS í”¼ë“œ", summary.get('rss_feeds', 0))
                    
                    st.write(f"**ë°ì´í„° í’ˆì§ˆ:** {summary.get('data_quality', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
                    if summary.get('fallback_used'):
                        st.warning("âš ï¸ í´ë°± ì½˜í…ì¸ ê°€ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤")
                    
                    if 'article_breakdown' in summary:
                        st.write("**ê¸°ì‚¬ ë¶„ì„:**")
                        for source, count in summary['article_breakdown'].items():
                            st.write(f"â€¢ {source}: {count}ê°œ")
                    
                    st.write("**ì²˜ë¦¬ ê³¼ì •:**")
                    for step in summary.get('processing_steps', []):
                        st.write(f"â€¢ {step}")
                
                with st.expander("ğŸ” í¬ë¡¤ë§ ê²€ì¦ ì •ë³´", expanded=False):
                    st.write("**ë””ë²„ê·¸:**")
                    for d in r['í¬ë¡¤ë§ ë””ë²„ê·¸']:
                        st.text(d)
                    st.write("**í¬ë¡¤ë§ëœ ê¸°ì‚¬ ë‚´ìš©:**")
                    for idx, art in enumerate(r['í¬ë¡¤ë§ëœ ê¸°ì‚¬'][:NUM_REFERENCES]): 
                        st.write(f"**ê¸°ì‚¬ {idx+1}:** {art[:200]}...")
                if r['ì°¸ê³  ë§í¬']:
                    st.write("**ì°¸ê³  ë§í¬:**")
                    for idx, l in enumerate(r['ì°¸ê³  ë§í¬'][:NUM_REFERENCES]): 
                        if l and l.startswith('http'):
                            st.markdown(f"ğŸ”— [ì°¸ê³ ìë£Œ {idx+1}]({l})")
                # LLMì´ ë¦¬ë·°í•œ ë‚´ìš© í† ê¸€ë¡œ í‘œì‹œ
                with st.expander("ğŸ“ LLM ë¦¬ë·° ë‚´ìš©", expanded=False):
                    st.write(r['ë¦¬ë·° ë‚´ìš©'])
                
                # ê°•í™”ëœ ë§í¬ ë¶„ì„ ì •ë³´ í‘œì‹œ
                if r.get('ë§í¬ ì¶”ì¶œ ë””ë²„ê·¸'):
                    with st.expander("ğŸ”— ê°•í™”ëœ ë§í¬ ë¶„ì„", expanded=False):
                        st.write("**ë§í¬ ì½˜í…ì¸  ì¶”ì¶œ ê³¼ì •:**")
                        for debug_item in r['ë§í¬ ì¶”ì¶œ ë””ë²„ê·¸']:
                            st.text(debug_item)
                        
                        st.info("ğŸ’¡ ìƒìœ„ ê´€ë ¨ì„± ë§í¬ì—ì„œ ì¶”ê°€ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•˜ì—¬ ë” ìƒì„¸í•œ íˆ¬ì ë¶„ì„ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.")
    
    # í¬ë¡¤ë§ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    st.subheader("ğŸ” í¬ë¡¤ë§ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    with st.expander("í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹¤í–‰", expanded=False):
        if st.button("í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘"):
            test_ticker = "AAPL"
            st.write(f"ğŸ§ª {test_ticker}ë¡œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            try:
                test_articles, test_links, test_debug = crawl_info_parallel(test_ticker, start_date)
                
                st.success(f"âœ… í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
                st.write(f"ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(test_articles)}ê°œ")
                st.write(f"ì°¸ê³  ë§í¬: {len(test_links)}ê°œ")
                st.write(f"ë””ë²„ê·¸ ì •ë³´: {len(test_debug)}ê°œ")
                
                if test_articles:
                    st.write("ì²« ë²ˆì§¸ ê¸°ì‚¬ ìƒ˜í”Œ:")
                    st.text(test_articles[0][:200])
                
                st.write("ë””ë²„ê·¸ ì •ë³´:")
                for debug_item in test_debug[:5]:
                    st.text(debug_item)
                    
            except Exception as e:
                st.error(f"âŒ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
                import traceback
                st.text(traceback.format_exc())

            # ë¶„ì„ ê²°ê³¼ ìƒì„¸ ë””ë²„ê¹… ì •ë³´ í‘œì‹œ
            with debug_container:
                if recommendation and "Error" not in recommendation:
                    st.success(f"ğŸ¯ {ticker} ì „ì²´ ë¶„ì„ ì„±ê³µ!")
                else:
                    st.error(f"ğŸš¨ {ticker} ë¶„ì„ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤")
                
                # ë¶„ì„ í†µê³„
                st.write("ğŸ“Š ë¶„ì„ í†µê³„:")
                st.write(f"   â€¢ í¬ë¡¤ë§ëœ ê¸°ì‚¬: {len(articles)}ê°œ")
                st.write(f"   â€¢ ì°¸ê³  ë§í¬: {len(links)}ê°œ")
                st.write(f"   â€¢ LLM ì‘ë‹µ ê¸¸ì´: {len(recommendation) if recommendation else 0}ì")
                st.write(f"   â€¢ ë°ì´í„° í’ˆì§ˆ: {crawling_summary.get('data_quality', 'ì•Œ ìˆ˜ ì—†ìŒ')}")


def generate_collected_info_summary(reasons, language, vllm_status):
    """Generate a summary of collected information in the selected language."""
    
    if language == "í•œêµ­ì–´":
        st.subheader("ğŸ“Š ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ í˜„í™©")
        
        if not vllm_status:
            st.error("âš ï¸ vLLM ì„œë²„ ì—°ê²° ë¬¸ì œë¡œ AI ë¶„ì„ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ì„ í†µí•´ vLLM ì„œë²„ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”.")
            st.info("ğŸ’¡ vLLM ì„œë²„ ì—†ì´ë„ í¬ë¡¤ë§ëœ ì •ë³´ëŠ” í™•ì¸í•  ìˆ˜ ìˆì§€ë§Œ, AI ê¸°ë°˜ íˆ¬ì ë¶„ì„ì€ ì œí•œë©ë‹ˆë‹¤.")
        
        # Overall statistics
        total_stocks = len(reasons)
        total_articles = sum(len(r.get('í¬ë¡¤ë§ëœ ê¸°ì‚¬', [])) for r in reasons)
        total_links = sum(len(r.get('ì°¸ê³  ë§í¬', [])) for r in reasons)
        successful_analyses = sum(1 for r in reasons if r.get('ì¶”ì²œ ì‚¬ìœ ') and "Error" not in str(r.get('ì¶”ì²œ ì‚¬ìœ ', '')))
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ë¶„ì„ ì™„ë£Œ ì¢…ëª©", f"{successful_analyses}/{total_stocks}")
        with col2:
            st.metric("ìˆ˜ì§‘ëœ ê¸°ì‚¬", total_articles)
        with col3:
            st.metric("ì°¸ê³  ë§í¬", total_links)
        with col4:
            success_rate = (successful_analyses / total_stocks * 100) if total_stocks > 0 else 0
            st.metric("ì„±ê³µë¥ ", f"{success_rate:.1f}%")
        
        # Data quality summary
        st.write("**ğŸ“ˆ ë°ì´í„° í’ˆì§ˆ ë¶„ì„:**")
        quality_counts = {}
        for r in reasons:
            quality = r.get('í¬ë¡¤ë§ ìš”ì•½', {}).get('data_quality', 'ì•Œ ìˆ˜ ì—†ìŒ')
            quality_counts[quality] = quality_counts.get(quality, 0) + 1
        
        for quality, count in quality_counts.items():
            percentage = (count / total_stocks * 100) if total_stocks > 0 else 0
            st.write(f"â€¢ {quality}: {count}ê°œ ì¢…ëª© ({percentage:.1f}%)")
        
        # Source-wise collection status
        st.write("**ğŸ” ì •ë³´ ì†ŒìŠ¤ë³„ ìˆ˜ì§‘ í˜„í™©:**")
        source_summary = {
            'Google Finance': 0, 'Yahoo Finance': 0, 'MarketWatch': 0, 
            'RSS í”¼ë“œ': 0, 'DuckDuckGo': 0
        }
        
        for r in reasons:
            crawling_summary = r.get('í¬ë¡¤ë§ ìš”ì•½', {})
            for source in source_summary.keys():
                source_key = source.lower().replace(' ', '_')
                source_summary[source] += crawling_summary.get(source_key, 0)
        
        for source, count in source_summary.items():
            st.write(f"â€¢ {source}: {count}ê°œ ê¸°ì‚¬")
        
        # Enhanced analysis information
        enhanced_count = sum(1 for r in reasons if r.get('ë§í¬ ì¶”ì¶œ ë””ë²„ê·¸'))
        if enhanced_count > 0:
            st.write("**ğŸ”— ê°•í™”ëœ ë§í¬ ë¶„ì„:**")
            st.write(f"â€¢ {enhanced_count}ê°œ ì¢…ëª©ì—ì„œ ì¶”ê°€ ë§í¬ ì½˜í…ì¸  ë¶„ì„ ì™„ë£Œ")
            st.info("ğŸ’¡ ìƒìœ„ ê´€ë ¨ì„± ë§í¬ì—ì„œ ì¶”ì¶œí•œ ì¶”ê°€ ì •ë³´ë¡œ ë” ì •í™•í•œ íˆ¬ì ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.")
        
        # Summary of issues if any
        error_count = sum(1 for r in reasons if "Error" in str(r.get('ì¶”ì²œ ì‚¬ìœ ', '')))
        if error_count > 0:
            st.warning(f"âš ï¸ {error_count}ê°œ ì¢…ëª©ì—ì„œ ë¶„ì„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            if not vllm_status:
                st.error("ğŸ”§ ëŒ€ë¶€ë¶„ì˜ ì˜¤ë¥˜ëŠ” vLLM ì„œë²„ ì—°ê²° ë¬¸ì œë¡œ ì¸í•œ ê²ƒì…ë‹ˆë‹¤. ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
    else:  # English
        st.subheader("ğŸ“Š Overall Data Collection Status")
        
        if not vllm_status:
            st.error("âš ï¸ vLLM server connection issues limit AI analysis capabilities. Please check system status to start the vLLM server.")
            st.info("ğŸ’¡ Crawled information is available without vLLM server, but AI-based investment analysis is limited.")
        
        # Overall statistics
        total_stocks = len(reasons)
        total_articles = sum(len(r.get('í¬ë¡¤ë§ëœ ê¸°ì‚¬', [])) for r in reasons)
        total_links = sum(len(r.get('ì°¸ê³  ë§í¬', [])) for r in reasons)
        successful_analyses = sum(1 for r in reasons if r.get('ì¶”ì²œ ì‚¬ìœ ') and "Error" not in str(r.get('ì¶”ì²œ ì‚¬ìœ ', '')))
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Analyzed Stocks", f"{successful_analyses}/{total_stocks}")
        with col2:
            st.metric("Articles Collected", total_articles)
        with col3:
            st.metric("Reference Links", total_links)
        with col4:
            success_rate = (successful_analyses / total_stocks * 100) if total_stocks > 0 else 0
            st.metric("Success Rate", f"{success_rate:.1f}%")
        
        # Data quality summary
        st.write("**ğŸ“ˆ Data Quality Analysis:**")
        quality_counts = {}
        for r in reasons:
            quality = r.get('í¬ë¡¤ë§ ìš”ì•½', {}).get('data_quality', 'Unknown')
            quality_counts[quality] = quality_counts.get(quality, 0) + 1
        
        for quality, count in quality_counts.items():
            percentage = (count / total_stocks * 100) if total_stocks > 0 else 0
            st.write(f"â€¢ {quality}: {count} stocks ({percentage:.1f}%)")
        
        # Source-wise collection status
        st.write("**ğŸ” Information Source Collection Status:**")
        source_summary = {
            'Google Finance': 0, 'Yahoo Finance': 0, 'MarketWatch': 0, 
            'RSS Feeds': 0, 'DuckDuckGo': 0
        }
        
        for r in reasons:
            crawling_summary = r.get('í¬ë¡¤ë§ ìš”ì•½', {})
            source_mapping = {
                'Google Finance': 'google_finance',
                'Yahoo Finance': 'yahoo_finance', 
                'MarketWatch': 'marketwatch',
                'RSS Feeds': 'rss_feeds',
                'DuckDuckGo': 'duckduckgo'
            }
            for source, key in source_mapping.items():
                source_summary[source] += crawling_summary.get(key, 0)
        
        for source, count in source_summary.items():
            st.write(f"â€¢ {source}: {count} articles")
        
        # Enhanced analysis information
        enhanced_count = sum(1 for r in reasons if r.get('ë§í¬ ì¶”ì¶œ ë””ë²„ê·¸'))
        if enhanced_count > 0:
            st.write("**ğŸ”— Enhanced Link Analysis:**")
            st.write(f"â€¢ {enhanced_count} stocks completed additional link content analysis")
            st.info("ğŸ’¡ Provides more accurate investment analysis with additional information extracted from top relevance links.")
        
        # Summary of issues if any
        error_count = sum(1 for r in reasons if "Error" in str(r.get('ì¶”ì²œ ì‚¬ìœ ', '')))
        if error_count > 0:
            st.warning(f"âš ï¸ {error_count} stocks encountered analysis errors.")
            if not vllm_status:
                st.error("ğŸ”§ Most errors are due to vLLM server connection issues. Please check system status.")
