# WSLì—ì„œ vLLMìœ¼ë¡œ AI í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„ê¸° ì‹¤í–‰í•˜ê¸°

## ğŸš€ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´
- **GPU**: NVIDIA GPU (CUDA ì§€ì›)
- **VRAM**: ìµœì†Œ 16GB (Llama-3.1-8B ëª¨ë¸ìš©)
- **RAM**: ìµœì†Œ 16GB
- **ì €ì¥ê³µê°„**: ìµœì†Œ 50GB

### ì†Œí”„íŠ¸ì›¨ì–´
- **WSL2**: Windows Subsystem for Linux 2
- **Ubuntu 20.04+** (WSL ë‚´)
- **Python 3.8+**
- **CUDA Toolkit**
- **NVIDIA Docker** (ì„ íƒì‚¬í•­)

## ğŸ› ï¸ ì„¤ì¹˜ ë° ì„¤ì •

### 1ë‹¨ê³„: WSL2 ë° Ubuntu ì„¤ì¹˜
```bash
# Windows PowerShell (ê´€ë¦¬ì ê¶Œí•œ)
wsl --install
wsl --set-default-version 2
```

### 2ë‹¨ê³„: Ubuntuì—ì„œ ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
# WSL Ubuntu í„°ë¯¸ë„ì—ì„œ
sudo apt update && sudo apt upgrade -y
sudo apt install -y python3 python3-pip python3-venv git curl jq
sudo apt install -y build-essential
```

### 3ë‹¨ê³„: NVIDIA GPU ë“œë¼ì´ë²„ ì„¤ì • (WSLìš©)
```bash
# NVIDIA Container Toolkit ì„¤ì¹˜
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt update
sudo apt install -y nvidia-container-toolkit
```

### 4ë‹¨ê³„: CUDA ì„¤ì¹˜
```bash
# CUDA Toolkit ì„¤ì¹˜
wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.0-1_all.deb
sudo dpkg -i cuda-keyring_1.0-1_all.deb
sudo apt update
sudo apt install -y cuda-toolkit-12-2
```

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1ë‹¨ê³„: í”„ë¡œì íŠ¸ í´ë”ë¡œ ì´ë™
```bash
cd /mnt/e/work/stock_app
```

### 2ë‹¨ê³„: ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
```bash
chmod +x *.sh
```

### 3ë‹¨ê³„: vLLM ì„œë²„ ì‹œì‘ (í„°ë¯¸ë„ 1)
```bash
./start_vllm.sh
```

### 4ë‹¨ê³„: Streamlit ì•± ì‹¤í–‰ (í„°ë¯¸ë„ 2)
```bash
./run_app_wsl.sh
```

### 5ë‹¨ê³„: ë¸Œë¼ìš°ì € ì ‘ì†
- http://localhost:8501

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë°©ë²•

### vLLM ì„œë²„ í…ŒìŠ¤íŠ¸
```bash
./test_vllm.sh
```

### Python ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
```bash
source venv/bin/activate
python test_functions.py
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### GPU ë©”ëª¨ë¦¬ ìµœì í™”
```bash
# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
```

### vLLM ì„œë²„ ì„¤ì • ìµœì í™”
```bash
# ê³ ì„±ëŠ¥ GPUìš© (A100, RTX 4090 ë“±)
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3.1-8B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 2 \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.9

# ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3.1-8B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 1 \
    --max-model-len 2048 \
    --gpu-memory-utilization 0.7
```

## ğŸ”§ ë¬¸ì œ í•´ê²°

### CUDA ì¸ì‹ ì•ˆë¨
```bash
# GPU ìƒíƒœ í™•ì¸
nvidia-smi
python3 -c "import torch; print(torch.cuda.is_available())"
```

### ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
```bash
# ìŠ¤ì™‘ ë©”ëª¨ë¦¬ ëŠ˜ë¦¬ê¸°
sudo fallocate -l 16G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

### vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨
```bash
# í¬íŠ¸ í™•ì¸
netstat -tulpn | grep 8000

# ë°©í™”ë²½ í™•ì¸
sudo ufw allow 8000
```

## ğŸ“ íŒŒì¼ êµ¬ì¡° (WSL ë²„ì „)
```
stock_app/
â”œâ”€â”€ stock_research_app.py      # ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ (vLLM ë²„ì „)
â”œâ”€â”€ requirements.txt           # ê¸°ë³¸ íŒ¨í‚¤ì§€
â”œâ”€â”€ requirements_vllm.txt      # vLLM ì „ìš© íŒ¨í‚¤ì§€
â”œâ”€â”€ start_vllm.sh             # vLLM ì„œë²„ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ run_app_wsl.sh            # WSLìš© ì•± ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ test_vllm.sh              # vLLM í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ test_functions.py         # Python ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
â””â”€â”€ README_WSL.md             # WSL ì‹¤í–‰ ê°€ì´ë“œ
```

## âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ì˜ˆìƒ ì„±ëŠ¥ (RTX 4090 ê¸°ì¤€)
- **ëª¨ë¸ ë¡œë”©**: 2-3ë¶„
- **ì²« ì¶”ë¡ **: 3-5ì´ˆ
- **ì´í›„ ì¶”ë¡ **: 1-2ì´ˆ
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 14-16GB VRAM

### ëª¨ë¸ ëŒ€ì•ˆ
- **ë” ì‘ì€ ëª¨ë¸**: Llama-3.1-7B
- **ë” ë¹ ë¥¸ ëª¨ë¸**: Qwen2-7B-Instruct
- **ì–‘ìí™” ëª¨ë¸**: Llama-3.1-8B-Instruct-AWQ
