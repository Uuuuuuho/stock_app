#!/bin/bash
# WSLì—ì„œ vLLM ì„œë²„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

echo "=== vLLM ì„œë²„ ì„¤ì • ë° ì‹¤í–‰ ==="

# # ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
# if [ ! -d "venv_vllm" ]; then
#     echo "ğŸ“¦ vLLMìš© ê°€ìƒí™˜ê²½ ìƒì„± ì¤‘..."
#     python3 -m venv venv_vllm
# fi

# echo "ğŸ”§ ê°€ìƒí™˜ê²½ í™œì„±í™”..."
# source venv_vllm/bin/activate

# # vLLM ì„¤ì¹˜
# echo "ğŸ“¥ vLLM íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘..."
# pip install --upgrade pip
# pip install vllm

# GPU í™•ì¸
echo "ğŸ” GPU ìƒíƒœ í™•ì¸..."
nvidia-smi

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„œë²„ ì‹œì‘
echo "ğŸš€ vLLM ì„œë²„ ì‹œì‘..."
echo "ëª¨ë¸: meta-llama/Llama-3.1-8B-Instruct"
echo "í¬íŠ¸: 8000"
echo "ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”."

python3 -m vllm.entrypoints.openai.api_server \
  --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --dtype float16 \
  --gpu-memory-utilization 0.8 \
  --max-num-seqs 1 \
  --max-num-batched-tokens 512
