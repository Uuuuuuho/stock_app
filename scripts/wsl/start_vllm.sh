#!/bin/bash
# WSLì—ì„œ vLLM ì„œë²„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

echo "=== vLLM ì„œë²„ ì„¤ì • ë° ì‹¤í–‰ ==="

# # ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
# if [ ! -d "venv_vllm" ]; then
#     echo "ğŸ“¦ vLLMìš© ê°€ìƒí™˜ê²½ ìƒì„± ì¤‘..."
#     python3 -m venv venv_vllm
# fi

# echo "ğŸ”§ ê°€ìƒí™˜ê²½ í™œì„±í™”..."
# source venv_vllm/bin/activate

# # vLLM ì„¤ì¹˜
# echo "ğŸ“¥ vLLM íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘..."
# pip install --upgrade pip
# pip install vllm

# GPU í™•ì¸
echo "ğŸ” GPU ìƒíƒœ í™•ì¸..."
nvidia-smi

export model_name="google/gemma-2b-it"
# export model_name="TinyLlama/TinyLlama-1.1B-Chat-v1.0"
# export model_name="mistralai/Mistral-7B-Instruct-v0.2"
# export model_name="meta-llama/Meta-Llama-3-8B-Instruct"
# export model_name="/mnt/e/work/gemma-3n-E4B-it/gemma-3n/models--google--gemma-3n-E4B-it/snapshots/e4c12697f6160380846ed13294cc7984c8c2ba9f"
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„œë²„ ì‹œì‘
echo "ğŸš€ vLLM ì„œë²„ ì‹œì‘..."
echo "ëª¨ë¸ ì´ë¦„: $model_name"
echo "í¬íŠ¸: 8000"
echo "ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”."

python3 -m vllm.entrypoints.openai.api_server \
  --model $model_name \
  --dtype auto \
  --gpu-memory-utilization 0.8 \
  --port 8000
  # --max-num-seqs 1 \
  # --max-num-batched-tokens 512 \
  # --max-model-len 2048