import streamlit as st
import yfinance as yf
import requests
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
import pandas as pd
import subprocess
from functools import lru_cache
import plotly.express as px
import plotly.graph_objects as go

# Fetch S&P500 tickers from Wikipedia
@lru_cache()
def get_sp500_tickers():
    tables = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')
    df = tables[0]
    return df['Symbol'].tolist()

def get_stock_data(start_date, end_date, target_return, top_n=5):
    sp500_tickers = get_sp500_tickers()
    stock_data = []
    for ticker in sp500_tickers:
        try:
            data = yf.Ticker(ticker).history(start=str(start_date), end=str(end_date))
            if data.empty:
                continue
            # ì‹œì‘ê°€ì™€ ì¢…ê°€
            start_price = data['Open'].iloc[0]
            end_price = data['Close'].iloc[-1]
            return_pct = (end_price - start_price) / start_price * 100
            # ë¦¬ìŠ¤í¬(ë³€ë™ì„±)
            daily_rets = data['Close'].pct_change().dropna()
            risk = daily_rets.std() * 100
            if return_pct >= target_return:
                stock_data.append({
                    "Ticker": ticker,
                    "Return (%)": return_pct,
                    "Risk (%)": risk
                })
        except Exception as e:
            print(f"Error fetching data for {ticker}: {e}")
    # ìˆ˜ìµë¥  ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ, ìƒìœ„ ì¢…ëª© ì„ íƒ
    stock_data = sorted(stock_data, key=lambda x: x['Return (%)'], reverse=True)[:top_n]
    return stock_data

def crawl_info(ticker, buy_date):
    search_engines = [
        f"https://www.google.com/search?q={ticker}+{buy_date}+stock+news",
        f"https://search.naver.com/search.naver?query={ticker}+{buy_date}+ì£¼ì‹+ë‰´ìŠ¤",
        f"https://www.youtube.com/results?search_query={ticker}+{buy_date}+stock+analysis"
    ]
    
    all_articles = []
    reference_links = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    for url in search_engines:
        try:
            response = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Google search results
            if 'google.com' in url:
                results = soup.find_all('div', class_='g')[:3]
                for res in results:
                    link_tag = res.find('a', href=True)
                    title_tag = res.find('h3')
                    snippet_tag = res.find('span', class_='aCOpRe')
                    if title_tag and snippet_tag and link_tag:
                        text = f"{title_tag.get_text()} - {snippet_tag.get_text()}"
                        all_articles.append(text)
                        reference_links.append(link_tag['href'])
            
            # Naver search results
            elif 'naver.com' in url:
                results = soup.find_all('div', class_='info_group')[:3]
                for res in results:
                    title_tag = res.find('a', class_='link_tit')
                    desc_tag = res.find('div', class_='dsc_wrap')
                    if title_tag and desc_tag:
                        text = f"{title_tag.get_text()} - {desc_tag.get_text()}"
                        all_articles.append(text)
                        reference_links.append(title_tag.get('href', 'https://search.naver.com'))
            
            # YouTube search results
            elif 'youtube.com' in url:
                results = soup.find_all('div', class_='ytd-video-renderer')[:2]
                for res in results:
                    title_tag = res.find('a', id='video-title')
                    if title_tag:
                        text = f"YouTube: {title_tag.get_text().strip()}"
                        all_articles.append(text)
                        video_link = title_tag.get('href', '')
                        if video_link.startswith('/watch'):
                            video_link = f"https://www.youtube.com{video_link}"
                        reference_links.append(video_link)
                        
        except Exception as e:
            print(f"Error crawling {url} for {ticker}: {e}")
            continue
    
    return (all_articles if all_articles else ["ì •ë³´ ì—†ìŒ"], 
            reference_links if reference_links else [])

def run_llm(ticker, buy_date, return_percentage, articles):
    prompt = f"""
    {buy_date}ì— {ticker}ì— íˆ¬ìí–ˆë‹¤ë©´ ì˜¤ëŠ˜ê¹Œì§€ ìˆ˜ìµë¥ ì€ {return_percentage:.2f}%ì…ë‹ˆë‹¤.
    ì•„ë˜ëŠ” ê·¸ ë‹¹ì‹œ ì£¼ìš” ê¸°ì‚¬ ë‚´ìš©ì…ë‹ˆë‹¤:
    {articles}
    ì´ ì¢…ëª©ì´ ìƒìŠ¹í•œ ì´ìœ ë¥¼ 3ì¤„ ì´ë‚´ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
    """
    try:
        result = subprocess.run(["ollama", "run", "llama3.1:8b"], input=prompt, text=True, capture_output=True)
        return result.stdout.strip()
    except Exception as e:
        print(f"Error running LLM for {ticker}: {e}")
        return "ì •ë³´ ì—†ìŒ"

def build_ui():
    # í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title="AI í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„ê¸°",
        page_icon="ğŸ“ˆ",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # í—¤ë” ì„¹ì…˜
    st.markdown("""
    <div style="background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); padding: 2rem; border-radius: 10px; margin-bottom: 2rem;">
        <h1 style="color: white; text-align: center; margin: 0;">ğŸ“ˆ AI ê¸°ë°˜ í¬íŠ¸í´ë¦¬ì˜¤ ì¶”ì²œ ë¦¬ì„œì¹˜ ë„ìš°ë¯¸</h1>
        <p style="color: white; text-align: center; margin: 0.5rem 0 0 0;">S&P500 ì¢…ëª© ë¶„ì„ ë° AI ê¸°ë°˜ íˆ¬ì ì‚¬ìœ  ë¶„ì„</p>
    </div>
    """, unsafe_allow_html=True)

    # ì‚¬ì´ë“œë°” ì…ë ¥ í¼
    with st.sidebar:
        st.header("ğŸ”§ ë¶„ì„ ì„¤ì •")
        
        with st.expander("ğŸ“… íˆ¬ì ê¸°ê°„ ì„¤ì •", expanded=True):
            start_date = st.date_input("íˆ¬ì ì‹œì‘ì¼:", value=pd.to_datetime("2022-01-01"))
            end_date = st.date_input("íˆ¬ì ì¢…ë£Œì¼:", value=pd.to_datetime("2022-06-01"))
        
        with st.expander("ğŸ¯ ë¶„ì„ ì¡°ê±´", expanded=True):
            target_return = st.slider("ëª©í‘œ ìˆ˜ìµë¥  (%)", min_value=0.0, max_value=100.0, value=10.0, step=0.5)
            top_n = st.select_slider("ì¶”ì²œ ì¢…ëª© ìˆ˜", options=list(range(1, 21)), value=5)
        
        st.markdown("---")
        analyze_button = st.button("ğŸš€ ë¶„ì„ ì‹¤í–‰", type="primary", use_container_width=True)
        
        if analyze_button:
            st.session_state.analyze = True

    if analyze_button or st.session_state.get('analyze', False):
        with st.spinner('ğŸ“Š S&P500 ì¢…ëª© ë°ì´í„° ë¶„ì„ ì¤‘...'):
            stock_data = get_stock_data(start_date, end_date, target_return, top_n)
        
        if stock_data:
            df = pd.DataFrame(stock_data)
            
            # ë©”íŠ¸ë¦­ ì¹´ë“œ
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ë¶„ì„ëœ ì¢…ëª© ìˆ˜", len(df), delta="ê°œ")
            with col2:
                st.metric("í‰ê·  ìˆ˜ìµë¥ ", f"{df['Return (%)'].mean():.1f}%")
            with col3:
                st.metric("ìµœê³  ìˆ˜ìµë¥ ", f"{df['Return (%)'].max():.1f}%")
            with col4:
                st.metric("í‰ê·  ë¦¬ìŠ¤í¬", f"{df['Risk (%)'].mean():.1f}%")
            
            st.markdown("---")
            
            # í…Œì´ë¸”ê³¼ ì°¨íŠ¸ ì„¹ì…˜
            col1, col2 = st.columns([1, 2])
            
            with col1:
                st.subheader("ğŸ“Š ì¢…ëª©ë³„ ìˆ˜ìµë¥  & ë¦¬ìŠ¤í¬")
                # í…Œì´ë¸” ë°ì´í„° í¬ë§·íŒ…
                display_df = df.copy()
                display_df['Return (%)'] = display_df['Return (%)'].round(2)
                display_df['Risk (%)'] = display_df['Risk (%)'].round(2)
                st.dataframe(display_df, use_container_width=True)
            
            with col2:
                st.subheader("ğŸ“ˆ í¬íŠ¸í´ë¦¬ì˜¤ ëˆ„ì  ìˆ˜ìµë¥ ")
                with st.spinner('ğŸ“ˆ í¬íŠ¸í´ë¦¬ì˜¤ ì„±ê³¼ ê³„ì‚° ì¤‘...'):
                    # Plotlyë¡œ ì¸í„°ë™í‹°ë¸Œ ì°¨íŠ¸ ìƒì„±
                    fig = go.Figure()
                    
                    for item in stock_data:
                        data = yf.Ticker(item['Ticker']).history(start=str(start_date), end=str(end_date))
                        cumulative_return = data['Close'].pct_change().add(1).cumprod()
                        
                        fig.add_trace(go.Scatter(
                            x=cumulative_return.index,
                            y=cumulative_return.values,
                            mode='lines',
                            name=item['Ticker'],
                            line=dict(width=2),
                            hovertemplate='<b>%{fullData.name}</b><br>' +
                                        'Date: %{x}<br>' +
                                        'Cumulative Return: %{y:.3f}<br>' +
                                        '<extra></extra>'
                        ))
                    
                    fig.update_layout(
                        title="ì¼ë³„ ëˆ„ì  ìˆ˜ìµë¥  ì¶”ì´",
                        xaxis_title="ë‚ ì§œ",
                        yaxis_title="ëˆ„ì  ìˆ˜ìµë¥ ",
                        hovermode='x unified',
                        height=600,
                        showlegend=True,
                        legend=dict(
                            orientation="h",
                            yanchor="bottom",
                            y=1.02,
                            xanchor="right",
                            x=1
                        )
                    )
                    
                    # í™•ëŒ€/ì¶•ì†Œ ë° ìŠ¤í¬ë¡¤ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
                    fig.update_xaxes(rangeslider_visible=True)
                    
                    st.plotly_chart(fig, use_container_width=True)

            # AI ë¶„ì„ ì„¹ì…˜
            st.markdown("---")
            st.header("ğŸ¤– AI íˆ¬ì ì‚¬ìœ  ë¶„ì„")
            
            # ì§„í–‰ìƒí™© í‘œì‹œ
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            reasons = []
            for i, item in enumerate(stock_data):
                status_text.text(f"ğŸ” {item['Ticker']} ì¢…ëª© ë¶„ì„ ì¤‘... ({i+1}/{len(stock_data)})")
                progress_bar.progress((i + 1) / len(stock_data))
                
                articles, links = crawl_info(item['Ticker'], start_date)
                reason = run_llm(item['Ticker'], start_date, item['Return (%)'], articles)
                reasons.append({
                    "Ticker": item['Ticker'], 
                    "Return (%)": round(item['Return (%)'], 2), 
                    "Risk (%)": round(item['Risk (%)'], 2), 
                    "ì¶”ì²œ ì‚¬ìœ ": reason,
                    "ì°¸ê³  ë§í¬": links
                })
            
            # ì™„ë£Œ í›„ ì§„í–‰ìƒí™© ìˆ¨ê¸°ê¸°
            progress_bar.empty()
            status_text.empty()
            
            # ê²°ê³¼ í…Œì´ë¸”ì„ ë” ë³´ê¸° ì¢‹ê²Œ í‘œì‹œ
            st.subheader("ğŸ“‹ ì¢…ëª© ì¶”ì²œ ìš”ì•½")
            
            for i, reason in enumerate(reasons):
                with st.expander(f"#{i+1} {reason['Ticker']} - ìˆ˜ìµë¥ : {reason['Return (%)']}%", expanded=i<3):
                    col1, col2 = st.columns([1, 3])
                    with col1:
                        st.metric("ìˆ˜ìµë¥ ", f"{reason['Return (%)']}%")
                        st.metric("ë¦¬ìŠ¤í¬", f"{reason['Risk (%)']}%")
                    with col2:
                        st.write("**AI ë¶„ì„ ê²°ê³¼:**")
                        st.write(reason['ì¶”ì²œ ì‚¬ìœ '])
                        
                        if reason['ì°¸ê³  ë§í¬']:
                            st.write("**ì°¸ê³  ë§í¬:**")
                            for idx, link in enumerate(reason['ì°¸ê³  ë§í¬'][:3]):
                                if link and link.startswith('http'):
                                    st.markdown(f"ğŸ”— [ì°¸ê³ ìë£Œ {idx+1}]({link})")
        else:
            st.error("âš ï¸ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ëª©í‘œ ìˆ˜ìµë¥ ì„ ë‚®ì¶°ë³´ì„¸ìš”.")

if __name__ == "__main__":
    build_ui()
